## 线性回归

### 概念

在机器学习中，线性回归是一种用于建立连续变量之间关系的统计学习方法。它可以帮助我们预测一个因变量和一个或多个自变量之间的线性关系。

具体而言，线性回归通过使用一个线性方程来描述这种关系，该方程将自变量的值映射到因变量的预测值。在一元线性回归中，只有一个自变量，而在多元线性回归中，有多个自变量。这些自变量可以是连续的，也可以是离散的，主要用来解决回归问题。

### 损失函数$J(\theta)$

一般回归问题使用最小二乘法来估计损失函数：
$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
$$

矩阵形式下：
$$
J(\theta) = \frac{1}{2m}(X\theta-y)^T*(X\theta-y)
$$
两边同时对$\theta$求导，求出$J(\theta)$的极小值。

### 梯度下降法（Gradient Descend）

对特征进行归一化，选择合适的学习率$\alpha$，递归的寻找最优解。

#### 学习率$\alpha$

决定每次步长大小，过小可能影响收敛速度，过大可能导致不收敛。

### 正规方程法（Normal Equation）

不需要特征缩放和递归计算，直接通过矩阵计算（下式未证明）：


$$
minJ(\theta): \theta = (X^T*X)^{-1}*X^T*y
$$
其中$(X^T*X)$可能不可逆，可能有两个原因：

1. 列向量线性相关，即训练集中存在冗余特征（线性相关的特征），此时应该剔除掉多余特征；
2. 特征过多，此时应该去掉影响较小的特征，或使用“正则化”。

### 对比

Normal Equation更简单粗暴，维度n规模较小时会更快，由于要计算矩阵的逆，时间复杂度近乎$O(n^3)$，因此在n到达$10^4$规模时需要慎重。

Gradient Descend会增加不必要的工作量，并且寻找合适的学习率比较繁琐，但是在维度n的规模很大时也会有很好的表现
